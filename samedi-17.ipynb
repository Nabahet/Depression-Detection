{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11699724,"sourceType":"datasetVersion","datasetId":7343598}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Chargement avec harmonisation des colonnes\ndef load_and_standardize_split(file_path):\n    df = pd.read_csv(file_path)\n    if 'PHQ_Binary' in df.columns:\n        df = df.rename(columns={'PHQ_Binary': 'PHQ8_Binary'})\n    return df\n\n# Charger les splits\ntrain_df = load_and_standardize_split(\"/kaggle/input/daic-woz/train_split_Depression_AVEC2017 (2).csv\")\ndev_df = load_and_standardize_split(\"/kaggle/input/daic-woz/dev_split_Depression_AVEC2017.csv\")\ntest_df = load_and_standardize_split(\"/kaggle/input/daic-woz/full_test_split.csv\")\n\n# Fusion\nfull_df = pd.concat([train_df, dev_df, test_df], ignore_index=True)\n\n# Générer les chemins\nbase_path = \"/kaggle/input/daic-woz\"\n\ndata = []\n\nfor _, row in full_df.iterrows():\n    pid = row[\"Participant_ID\"]\n    folder_name = f\"{pid}_P\"\n    \n    audio_path = os.path.join(base_path, folder_name, f\"{pid}_AUDIO.wav\")\n    transcript_path = os.path.join(base_path, folder_name, f\"{pid}_TRANSCRIPT.csv\")\n    \n    data.append({\n        \"Participant_ID\": pid,\n        \"audio_path\": audio_path,\n        \"transcript_path\": transcript_path,\n        \"PHQ8_Binary\": row[\"PHQ8_Binary\"]\n    })\n\n# Sauvegarder\ndaic_paths_df = pd.DataFrame(data)\ndaic_paths_df.to_csv(\"daic_paths.csv\", index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:22:41.765151Z","iopub.execute_input":"2025-05-17T20:22:41.766110Z","iopub.status.idle":"2025-05-17T20:22:41.806407Z","shell.execute_reply.started":"2025-05-17T20:22:41.766077Z","shell.execute_reply":"2025-05-17T20:22:41.805389Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport nltk\nfrom tqdm import tqdm\n\nnltk.download('punkt')\n\n# === LOAD GLOVE EMBEDDINGS ===\ndef load_glove_model(file_path):\n    model = {}\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            split_line = line.strip().split()\n            word = split_line[0]\n            vector = np.asarray(split_line[1:], dtype='float32')\n            model[word] = vector\n    return model\n\n# === AUDIO FUNCTIONS ===\ndef get_patient_timestamps(transcript_path):\n    df = pd.read_csv(transcript_path, sep=\"\\t\")\n    df.columns = df.columns.str.lower()\n    df = df[df['speaker'].str.lower() == 'participant']\n    return list(df[['start_time', 'stop_time']].itertuples(index=False, name=None))\n\ndef load_patient_audio(audio_path, timestamps):\n    y, sr = librosa.load(audio_path, sr=16000)\n    patient_audio = [y[int(start * sr):int(end * sr)] for start, end in timestamps]\n    return np.concatenate(patient_audio), sr\n\ndef segment_audio(audio, sr, segment_duration=7.6):\n    segment_samples = int(segment_duration * sr)\n    segments = [audio[i:i+segment_samples] for i in range(0, len(audio), segment_samples)]\n    if len(segments[-1]) < segment_samples:\n        segments = segments[:-1]\n    return segments\n\ndef augment_audio(segments, noise_factor=0.005):\n    return [seg + noise_factor * np.random.randn(len(seg)) for seg in segments]\n\ndef extract_mfcc_audio(segments, sr, n_mfcc=60, n_fft=1024, hop_length=322, win_length=960):\n    return [librosa.feature.mfcc(y=seg, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft,\n                                 hop_length=hop_length, win_length=win_length).T for seg in segments]\n\n# === TEXT FUNCTIONS ===\ndef segment_transcripts(transcript_path, segment_duration=7.6):\n    df = pd.read_csv(transcript_path, sep='\\t')\n    df = df.dropna(subset=['value'])\n    total_time = df['stop_time'].max()\n    n_segments = int(np.ceil(total_time / segment_duration))\n    segments = []\n\n    for i in range(n_segments):\n        start, end = i * segment_duration, (i + 1) * segment_duration\n        segment = df[(df['start_time'] >= start) & (df['stop_time'] < end)]\n        text = \" \".join(segment['value'].astype(str).tolist())\n        segments.append(text)\n\n    return segments\n\ndef text_to_embedding(text, model, max_words=9, emb_size=100):\n    words = text.split()\n    vecs = [model.get(w.lower(), np.zeros(emb_size)) for w in words[:max_words]]\n    while len(vecs) < max_words:\n        vecs.append(np.zeros(emb_size))\n    emb = np.stack(vecs, axis=0).T  # shape (100, 9)\n    if emb.shape[0] < 378:\n        pad = np.zeros((378 - emb.shape[0], emb.shape[1]))\n        emb = np.vstack((emb, pad))\n    return emb  # shape (378, 9)\n\n# === TRAITEMENT PAR PATIENT ===\ndef process_patient(audio_path, transcript_path, glove_model):\n    try:\n        timestamps = get_patient_timestamps(transcript_path)\n        patient_audio, sr = load_patient_audio(audio_path, timestamps)\n        audio_segments = segment_audio(patient_audio, sr)\n        audio_segments = augment_audio(audio_segments)\n        mfcc_audio = extract_mfcc_audio(audio_segments, sr)\n\n        text_segments = segment_transcripts(transcript_path)\n        mfcc_text = [text_to_embedding(t, glove_model) for t in text_segments]\n\n        n_segments = min(len(mfcc_audio), len(mfcc_text))\n        return mfcc_audio[:n_segments], mfcc_text[:n_segments]\n    except Exception as e:\n        print(f\"Erreur patient {audio_path} : {e}\")\n        return [], []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PIPELINE PRINCIPAL ===\ndef build_dataset(csv_path, glove_path, output_path=\"processed_daic_dataset.npz\"):\n    df = pd.read_csv(csv_path)\n    glove_model = load_glove_model(glove_path)\n\n    all_audio = []\n    all_text = []\n    all_labels = []\n    all_ids = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        audio_path = row[\"audio_path\"]\n        transcript_path = row[\"transcript_path\"]\n        label = row[\"PHQ8_Binary\"]\n        patient_id = row[\"id\"]\n\n        mfcc_audio, mfcc_text = process_patient(audio_path, transcript_path, glove_model)\n\n        for a, t in zip(mfcc_audio, mfcc_text):\n            all_audio.append(a)\n            all_text.append(t)\n            all_labels.append(label)\n            all_ids.append(patient_id)\n\n    np.savez_compressed(output_path,\n                        ids=np.array(all_ids),\n                        X_audio=np.array(all_audio, dtype=object),\n                        X_text=np.array(all_text, dtype=object),\n                        y=np.array(all_labels))\n    \n    print(f\"✅ Dataset sauvegardé : {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndata = np.load(\"processed_daic_dataset.npz\", allow_pickle=True)\nX_audio = data[\"audio\"]      # shape: (n_samples, 378, 60)\nX_text = data[\"text\"]        # shape: (n_samples, 378, 9)\ny = data[\"labels\"]           # shape: (n_samples, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split train (70%) and temp (30%)\nX_audio_train, X_audio_temp, X_text_train, X_text_temp, y_train, y_temp = train_test_split(\n    X_audio, X_text, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Split temp (30%) into dev (15%) and test (15%)\nX_audio_dev, X_audio_test, X_text_dev, X_text_test, y_dev, y_test = train_test_split(\n    X_audio_temp, X_text_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}