{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ee9195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:48:52.305162Z",
     "iopub.status.busy": "2025-05-11T13:48:52.304526Z",
     "iopub.status.idle": "2025-05-11T13:48:58.378880Z",
     "shell.execute_reply": "2025-05-11T13:48:58.378021Z"
    },
    "papermill": {
     "duration": 6.079359,
     "end_time": "2025-05-11T13:48:58.380153",
     "exception": false,
     "start_time": "2025-05-11T13:48:52.300794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\r\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\r\n",
      "Building wheels for collected packages: langdetect\r\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=f914239b9bf6fc1dce2b227f03829d5952c0d89406035ba17e76f258c91be45b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\r\n",
      "Successfully built langdetect\r\n",
      "Installing collected packages: langdetect\r\n",
      "Successfully installed langdetect-1.0.9\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b793d3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:48:58.387767Z",
     "iopub.status.busy": "2025-05-11T13:48:58.387107Z",
     "iopub.status.idle": "2025-05-11T13:49:01.727899Z",
     "shell.execute_reply": "2025-05-11T13:49:01.726870Z"
    },
    "papermill": {
     "duration": 3.345917,
     "end_time": "2025-05-11T13:49:01.729296",
     "exception": false,
     "start_time": "2025-05-11T13:48:58.383379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language_tool_python\r\n",
      "  Downloading language_tool_python-2.9.3-py3-none-any.whl.metadata (54 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (7.0.0)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (0.10.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2025.1.31)\r\n",
      "Downloading language_tool_python-2.9.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: language_tool_python\r\n",
      "Successfully installed language_tool_python-2.9.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1caf93d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:01.737006Z",
     "iopub.status.busy": "2025-05-11T13:49:01.736466Z",
     "iopub.status.idle": "2025-05-11T13:49:15.571950Z",
     "shell.execute_reply": "2025-05-11T13:49:15.571308Z"
    },
    "papermill": {
     "duration": 13.840601,
     "end_time": "2025-05-11T13:49:15.573298",
     "exception": false,
     "start_time": "2025-05-11T13:49:01.732697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 13:49:04.569979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746971344.723393      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746971344.767513      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# !pip install librosa nltk language-tool-python fasttext\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import nltk\n",
    "import re\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import language_tool_python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Concatenate, Flatten\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9004b502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:15.581134Z",
     "iopub.status.busy": "2025-05-11T13:49:15.580499Z",
     "iopub.status.idle": "2025-05-11T13:49:15.835584Z",
     "shell.execute_reply": "2025-05-11T13:49:15.834851Z"
    },
    "papermill": {
     "duration": 0.260065,
     "end_time": "2025-05-11T13:49:15.836853",
     "exception": false,
     "start_time": "2025-05-11T13:49:15.576788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase, remove numbers/punctuation, grammar correct\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c52d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:15.845301Z",
     "iopub.status.busy": "2025-05-11T13:49:15.845070Z",
     "iopub.status.idle": "2025-05-11T13:49:15.869341Z",
     "shell.execute_reply": "2025-05-11T13:49:15.868849Z"
    },
    "papermill": {
     "duration": 0.029008,
     "end_time": "2025-05-11T13:49:15.870330",
     "exception": false,
     "start_time": "2025-05-11T13:49:15.841322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_patient_timestamps(transcript_path):\n",
    "    df = pd.read_csv(transcript_path, sep='\\t')\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    df['speaker'] = df['speaker'].str.strip().str.lower()\n",
    "    patient_segments = df[df['speaker'] == 'participant'][['start_time', 'stop_time']]\n",
    "    return list(patient_segments.itertuples(index=False, name=None))\n",
    "\n",
    "def load_patient_audio(audio_path, timestamps):\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "    patient_audio = []\n",
    "    for start, stop in timestamps:\n",
    "        start_sample = int(start * sr)\n",
    "        stop_sample = int(stop * sr)\n",
    "        if start_sample < stop_sample:  # avoid invalid segments\n",
    "            patient_audio.append(audio[start_sample:stop_sample])\n",
    "    if not patient_audio:\n",
    "        raise ValueError(\"No valid participant audio segments found.\")\n",
    "    patient_audio = np.concatenate(patient_audio)\n",
    "    return patient_audio, sr\n",
    "\n",
    "def segment_audio(audio, sr, segment_length=7.6):\n",
    "    segment_samples = int(segment_length * sr)\n",
    "    segments = []\n",
    "    for start in range(0, len(audio), segment_samples):\n",
    "        end = start + segment_samples\n",
    "        if end <= len(audio):\n",
    "            segments.append(audio[start:end])\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cab99",
   "metadata": {
    "papermill": {
     "duration": 0.002919,
     "end_time": "2025-05-11T13:49:15.876330",
     "exception": false,
     "start_time": "2025-05-11T13:49:15.873411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **TEXT******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0e0c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:15.883860Z",
     "iopub.status.busy": "2025-05-11T13:49:15.883163Z",
     "iopub.status.idle": "2025-05-11T13:49:25.275814Z",
     "shell.execute_reply": "2025-05-11T13:49:25.274996Z"
    },
    "papermill": {
     "duration": 9.397758,
     "end_time": "2025-05-11T13:49:25.277288",
     "exception": false,
     "start_time": "2025-05-11T13:49:15.879530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe model\n",
    "def load_glove_model(file_path):\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split_line = line.strip().split()\n",
    "            word = split_line[0]\n",
    "            vector = np.asarray(split_line[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "# Load the GloVe embeddings (replace 'path_to_glove_file' with the actual file path)\n",
    "glove_model = load_glove_model('/kaggle/input/glove-model/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8acf26d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:25.284502Z",
     "iopub.status.busy": "2025-05-11T13:49:25.284276Z",
     "iopub.status.idle": "2025-05-11T13:49:25.400019Z",
     "shell.execute_reply": "2025-05-11T13:49:25.399423Z"
    },
    "papermill": {
     "duration": 0.120471,
     "end_time": "2025-05-11T13:49:25.401158",
     "exception": false,
     "start_time": "2025-05-11T13:49:25.280687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate GloVe model (dict) with 100D embeddings\n",
    "glove_model = {\"this\": np.random.rand(100), \"is\": np.random.rand(100),\n",
    "               \"a\": np.random.rand(100), \"test\": np.random.rand(100),\n",
    "               \"sentence\": np.random.rand(100), \"for\": np.random.rand(100),\n",
    "               \"GloVe\": np.random.rand(100), \"embeddings\": np.random.rand(100)}\n",
    "\n",
    "def text_to_embedding(text, max_words=9):\n",
    "    words = text.split()\n",
    "    vecs = [glove_model.get(w.lower(), np.zeros(100)) for w in words[:max_words]]\n",
    "    \n",
    "    # Pad with zero vectors if there are fewer than `max_words` words\n",
    "    while len(vecs) < max_words:\n",
    "        vecs.append(np.zeros(100))\n",
    "    \n",
    "    # Stack into (9, 100), then transpose to (100, 9)\n",
    "    emb = np.stack(vecs, axis=0).T  # (100, 9)\n",
    "    \n",
    "    # Pad to (378, 9)\n",
    "    if emb.shape[0] < 378:\n",
    "        pad_rows = np.zeros((378 - emb.shape[0], emb.shape[1]))\n",
    "        emb = np.vstack((emb, pad_rows))  # (378, 9)\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cfdb9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:25.408301Z",
     "iopub.status.busy": "2025-05-11T13:49:25.408061Z",
     "iopub.status.idle": "2025-05-11T13:49:28.165224Z",
     "shell.execute_reply": "2025-05-11T13:49:28.164513Z"
    },
    "papermill": {
     "duration": 2.761931,
     "end_time": "2025-05-11T13:49:28.166383",
     "exception": false,
     "start_time": "2025-05-11T13:49:25.404452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 9)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = clean_text(\"This is a test sentence for GloVe embeddings\")\n",
    "embedding = text_to_embedding(text)\n",
    "print(embedding.shape)  # Expected output: (378, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4e8360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:28.173577Z",
     "iopub.status.busy": "2025-05-11T13:49:28.173342Z",
     "iopub.status.idle": "2025-05-11T13:49:28.178220Z",
     "shell.execute_reply": "2025-05-11T13:49:28.177667Z"
    },
    "papermill": {
     "duration": 0.009696,
     "end_time": "2025-05-11T13:49:28.179352",
     "exception": false,
     "start_time": "2025-05-11T13:49:28.169656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_early_fusion_LSTM(input_shape_audio=(378,60), input_shape_text=(378,9)):\n",
    "    input_audio = Input(shape=input_shape_audio)\n",
    "    x_audio = LSTM(60, return_sequences=True)(input_audio)\n",
    "    x_audio = BatchNormalization()(x_audio)\n",
    "    x_audio = Dropout(0.2)(x_audio)\n",
    "    x_audio = Flatten()(x_audio)\n",
    "\n",
    "    x_text = Input(shape=input_shape_text)\n",
    "    #x_text = LSTM(60, return_sequences=True)(input_text)\n",
    "    #x_text = BatchNormalization()(x_text)\n",
    "    #x_text = Dropout(0.2)(x_text)\n",
    "    #x_text = Flatten()(x_text)\n",
    "\n",
    "    merged = Concatenate()([x_audio, x_text])\n",
    "    dense = Dense(15, activation='tanh')(merged)\n",
    "    dense = Dense(10, activation='tanh')(dense)\n",
    "    output = Dense(2, activation='sigmoid')(dense)\n",
    "\n",
    "    return Model(inputs=[input_audio, input_text], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2d0c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:28.186437Z",
     "iopub.status.busy": "2025-05-11T13:49:28.186228Z",
     "iopub.status.idle": "2025-05-11T13:49:28.191213Z",
     "shell.execute_reply": "2025-05-11T13:49:28.190703Z"
    },
    "papermill": {
     "duration": 0.009692,
     "end_time": "2025-05-11T13:49:28.192189",
     "exception": false,
     "start_time": "2025-05-11T13:49:28.182497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_level_fusion_LSTM(input_shape_audio=(378,60), input_shape_text=(378,9)):\n",
    "    input_audio = Input(shape=input_shape_audio)\n",
    "    x_audio = LSTM(60, return_sequences=True)(input_audio)\n",
    "    x_audio = BatchNormalization()(x_audio)\n",
    "    x_audio = Dropout(0.2)(x_audio)\n",
    "\n",
    "    x_text = Input(shape=input_shape_text)\n",
    "    #x_text = LSTM(60, return_sequences=True)(input_text)\n",
    "    #x_text = LSTM(40, return_sequences=True)(x_text)\n",
    "    #x_text = LSTM(20)(x_text)\n",
    "\n",
    "    fusion = Concatenate()([x_audio, x_text])\n",
    "    fusion = LSTM(60, return_sequences=True)(fusion)\n",
    "    fusion = BatchNormalization()(fusion)\n",
    "    fusion = Dropout(0.2)(fusion)\n",
    "    fusion = Flatten()(fusion)\n",
    "    fusion = Dropout(0.2)(fusion)\n",
    "    fusion = Dense(15, activation='tanh')(fusion)\n",
    "    fusion = Dense(10, activation='tanh')(fusion)\n",
    "    output = Dense(2, activation='sigmoid')(fusion)\n",
    "\n",
    "    return Model(inputs=[input_audio, input_text], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bd0d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:49:28.199174Z",
     "iopub.status.busy": "2025-05-11T13:49:28.198581Z",
     "iopub.status.idle": "2025-05-11T13:49:28.494005Z",
     "shell.execute_reply": "2025-05-11T13:49:28.493055Z"
    },
    "papermill": {
     "duration": 0.299894,
     "end_time": "2025-05-11T13:49:28.495017",
     "exception": true,
     "start_time": "2025-05-11T13:49:28.195123",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model_level_fusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/906318928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_level_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_audio_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_text_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model_level_fusion' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model_level_fusion()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit([X_audio_train, X_text_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02201697",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Concatenate\n",
    "\n",
    "def build_early_fusion_CNN(input_shape_audio=(378, 60), input_shape_text=(378, 9)):\n",
    "    # Audio Input\n",
    "    input_audio = Input(shape=input_shape_audio)\n",
    "    \n",
    "    # Audio CNN Layers\n",
    "    x_audio = Conv2D(128, (3, 3), activation='relu')(input_audio)\n",
    "    x_audio = MaxPooling2D(pool_size=(2, 2))(x_audio)\n",
    "    \n",
    "    x_audio = Conv2D(64, (3, 3), activation='relu')(x_audio)\n",
    "    x_audio = MaxPooling2D(pool_size=(2, 2))(x_audio)\n",
    "    x_audio = Dropout(0.02)(x_audio)\n",
    "    \n",
    "    x_audio = Flatten()(x_audio)\n",
    "\n",
    "    # Text Input\n",
    "    x_text = Input(shape=input_shape_text)\n",
    "    \n",
    "    # Text CNN Layers\n",
    "    #x_text = Conv2D(128, (3, 3), activation='relu')(input_text)\n",
    "    #x_text = MaxPooling2D(pool_size=(2, 2))(x_text)\n",
    "    #x_text = Dropout(0.02)(x_text)\n",
    "    \n",
    "    #x_text = Conv2D(64, (3, 3), activation='relu')(x_text)\n",
    "    #x_text = MaxPooling2D(pool_size=(2, 2))(x_text)\n",
    "    #x_text = Dropout(0.02)(x_text)\n",
    "    \n",
    "    x_text = Flatten()(x_text)\n",
    "\n",
    "    # Early Fusion (Concatenate the audio and text features)\n",
    "    merged = Concatenate()([x_audio, x_text])\n",
    "    \n",
    "    # Fully connected layers after fusion\n",
    "    dense = Dense(15, activation='tanh')(merged)\n",
    "    #dense = Dropout(0.02)(dense)  # Dropout to reduce overfitting\n",
    "    dense = Dense(10, activation='tanh')(dense)\n",
    "    output = Dense(2, activation='sigmoid')(dense)  # Assuming a binary classification task\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[input_audio, input_text], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0811",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_shape_audio = (378, 60)  # Example input shape for the audio (378x60 spectrogram)\n",
    "input_shape_text = (378, 9)    # Example input shape for the text (9 words)\n",
    "\n",
    "model = build_early_fusion_CNN(input_shape_audio, input_shape_text)\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7343598,
     "sourceId": 11699724,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7387964,
     "sourceId": 11768119,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.200387,
   "end_time": "2025-05-11T13:49:31.563524",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T13:48:48.363137",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
